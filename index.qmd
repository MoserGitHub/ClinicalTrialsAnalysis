# Introduction

This web book serves as a documentation source of some analysis approaches I used during my work at CTU Bern. I learn when I write it down (and experiment) actively. My notes contain errors, because I am a human being. Any notification of errors is highly appreciated.

## General notation and abbreviations

-   iid: Independent and identically distributed.
-   pdf: Probability density function. Most often denoted as $f(\cdot)$. For bivariate pdf we use the notation $f_2(\cdot)$.
-   cdf: Cumulative distribution function. Most often denoted as $F(\cdot)$. For bivariate pdf we use the notation $F_2(\cdot)$.
-   $N_2$: Bivariate cdf of the Gaussian distribution.
-   $\phi$: pdf of the standard Gaussian distribution.
-   $\Phi$: cdf of the standard Gaussian distribution.
-   $\Phi^{-1}$: Quantile function of the standard Gaussian distribution function.

## 'Power' vocabulary

In their supplement Kunzmann et al. @Kunzmann2022 provide a literature review of the terminology used in articles. We provide here a summary of this terminology:

-   **Frequentist power**: Probability of rejection given that the alternative hypothesis is true.
-   **Average power**: Prior averaged probability of rejection. Often also called 'probability of success', 'assurance', 'Bayesian predictive power'.
-   **Prior adjusted power**: Joint probability of rejection and that the treatment effect is effective.

## Some 'Bayesian' concepts

- **Prior**: A random variable $\Theta$ with pdf $f(\theta)$ representing that the uncertainty of a parameter of interest.
- **Design prior**: Prior used before data collection as data generating mode @Stefan2019.
- **Analysis prior**: Prior used for Bayesian analysis of the collected data @Stefan2019.
- **Prior predictive distribution**: Situation *before* a sample was taken. Let $\theta$ be a realisation of a random variable $\Theta$ with pdf $f(\theta)$. Then for a future observation $\tilde X$ the pdf is
$$
f(\tilde x)=\int_\Theta f(\tilde x, \theta)d\theta=\int_\Theta \underbrace{f(\tilde x | \theta)}_{likelihood}\underbrace{f(\theta)}_{prior}d\theta.
$$
-   **Posterior predictive distribution**: Situation *after* a sample was taken. Let $\theta$ be a realisation of a random variable $\Theta$ with pdf $f(\theta)$. Then for a future observation $\tilde X$ and observed $X$ (since $X$ is independent $\tilde X$) the pdf is
$$
f(\tilde x|x)=\int_\Theta f(\tilde x | \theta, x)f(\theta|x)d\theta=\int_\Theta \underbrace{f(\tilde x | \theta)}_{likelihood}\underbrace{f(\theta|x)}_{posterior}d\theta.
$$
- **Improper prior**: A prior with $\int_\Theta f(\theta)=\infty$.
- **Jeffrey's prior**: For an unknown parameter $\theta$ Jeffrey's (scalar) prior is defined as $f(\theta) \propto \sqrt{I(\theta)}$, where $I(\theta)$ is the expected Fisher information of $\theta$ @Held2020. Jeffrey's prior can be improper @Held2020. Bayesian point estimates using Jeffrey's prior are often very close to maximum likelihood estimators @Held2020.

------------------------------------------------------------------------

**Example: Jeffrey's prior for the binomial model**

The likelihood of the binomial model is
$$
f(x|\theta)=\begin{pmatrix} n \\ x \end{pmatrix}\theta^x(1-\theta)^{n-x}
$$
and thus
$$
L:=log(f(x|\theta))= x\log(\theta)+(n-x)\log(1-\theta).
$$
Simple algebra leads to
$$
\frac{dL}{d\theta}=\frac{x}{\theta}-\frac{n-x}{1-\theta}, \quad \frac{d^2L}{d\theta^2}=-\frac{x}{\theta^2}-\frac{n-x}{(1-\theta)^2}.
$$
The expected Fisher information is
$$
I(\theta)=-E_\theta\left( \frac{d^2L}{d\theta^2} \right)=\frac{n\theta}{\theta^2}+\frac{n-n\theta}{(1-\theta)^2}=\frac{n}{\theta(1-\theta)}\propto\frac{1}{\theta(1-\theta)}.
$$
Thus, Jeffrey's prior for the binomial model is
$$
f(\theta) \propto \frac{1}{\sqrt{\theta(1-\theta)}}=\theta^{-0.5}(1-\theta)^{-0.5}=beta(0.5, 0.5).
$$
Jeffrey's prior for the binomial model is a proper prior @Held2020.

------------------------------------------------------------------------

## Used R libraries

```{r lib}
#| echo: true
#| message: false
#| warning: false
#| code-fold: show

# All tidyverse functions
library(tidyverse)
# For frequentist sample size calculation
library(epiR)
# Multivariate Gaussian and t-distributions
library(mvtnorm)
# Beta binomial distribution
library(extraDistr)
# Agresti-Caffo confidence intervals for risk difference
library(PropCIs)
```